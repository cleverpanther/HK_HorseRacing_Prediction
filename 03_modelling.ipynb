{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Classification Modelling\n",
    " 1. Classify if horse can win 1st position\n",
    " 2. Classify if horse can win top 3 positions\n",
    " 3. Classify if horse can be ranked in the top half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test files\n",
    "df_train = pd.read_csv('df_train.csv', index_col=0)\n",
    "df_test = pd.read_csv('df_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24155, 27)\n",
      "(5209, 27)\n"
     ]
    }
   ],
   "source": [
    "# View the shape of the train and test files\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finishing_position</th>\n",
       "      <th>horse_number</th>\n",
       "      <th>horse_name</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey</th>\n",
       "      <th>trainer</th>\n",
       "      <th>actual_weight</th>\n",
       "      <th>declared_horse_weight</th>\n",
       "      <th>draw</th>\n",
       "      <th>length_behind_winner</th>\n",
       "      <th>...</th>\n",
       "      <th>running_position_6</th>\n",
       "      <th>race_id</th>\n",
       "      <th>recent_6_runs</th>\n",
       "      <th>recent_ave_rank</th>\n",
       "      <th>race_distance</th>\n",
       "      <th>HorseWin</th>\n",
       "      <th>HorseRankTop3</th>\n",
       "      <th>HorseRankTop50Percent</th>\n",
       "      <th>jockey_ave_rank</th>\n",
       "      <th>trainer_ave_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DOUBLE DRAGON</td>\n",
       "      <td>K019</td>\n",
       "      <td>B Prebble</td>\n",
       "      <td>D Cruz</td>\n",
       "      <td>133</td>\n",
       "      <td>1032</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-001</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.081266</td>\n",
       "      <td>7.393481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>PLAIN BLUE BANNER</td>\n",
       "      <td>S070</td>\n",
       "      <td>D Whyte</td>\n",
       "      <td>D E Ferraris</td>\n",
       "      <td>133</td>\n",
       "      <td>1075</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-001</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.848828</td>\n",
       "      <td>6.611340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   finishing_position  horse_number         horse_name horse_id     jockey  \\\n",
       "0                   1           1.0      DOUBLE DRAGON     K019  B Prebble   \n",
       "1                   2           2.0  PLAIN BLUE BANNER     S070    D Whyte   \n",
       "\n",
       "        trainer  actual_weight  declared_horse_weight  draw  \\\n",
       "0        D Cruz            133                   1032     1   \n",
       "1  D E Ferraris            133                   1075    13   \n",
       "\n",
       "  length_behind_winner  ...  running_position_6   race_id  recent_6_runs  \\\n",
       "0                    -  ...                 NaN  2014-001              1   \n",
       "1                    2  ...                 NaN  2014-001              2   \n",
       "\n",
       "   recent_ave_rank race_distance  HorseWin  HorseRankTop3  \\\n",
       "0              1.0          1400         1              1   \n",
       "1              2.0          1400         0              1   \n",
       "\n",
       "   HorseRankTop50Percent jockey_ave_rank trainer_ave_rank  \n",
       "0                      1        6.081266         7.393481  \n",
       "1                      1        5.848828         6.611340  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 2 rows of the train file\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finishing_position</th>\n",
       "      <th>horse_number</th>\n",
       "      <th>horse_name</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>jockey</th>\n",
       "      <th>trainer</th>\n",
       "      <th>actual_weight</th>\n",
       "      <th>declared_horse_weight</th>\n",
       "      <th>draw</th>\n",
       "      <th>length_behind_winner</th>\n",
       "      <th>...</th>\n",
       "      <th>running_position_6</th>\n",
       "      <th>race_id</th>\n",
       "      <th>recent_6_runs</th>\n",
       "      <th>recent_ave_rank</th>\n",
       "      <th>race_distance</th>\n",
       "      <th>HorseWin</th>\n",
       "      <th>HorseRankTop3</th>\n",
       "      <th>HorseRankTop50Percent</th>\n",
       "      <th>jockey_ave_rank</th>\n",
       "      <th>trainer_ave_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24155</th>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GLORIOUS PARTNERS</td>\n",
       "      <td>A089</td>\n",
       "      <td>T H So</td>\n",
       "      <td>D J Hall</td>\n",
       "      <td>125</td>\n",
       "      <td>1112</td>\n",
       "      <td>9</td>\n",
       "      <td>9-1/4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-377</td>\n",
       "      <td>11/10</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.331781</td>\n",
       "      <td>6.638824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24156</th>\n",
       "      <td>12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>APPLAUSE</td>\n",
       "      <td>A023</td>\n",
       "      <td>O Doleuze</td>\n",
       "      <td>K L Man</td>\n",
       "      <td>124</td>\n",
       "      <td>1067</td>\n",
       "      <td>7</td>\n",
       "      <td>10-1/2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-377</td>\n",
       "      <td>12/13/12</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.819133</td>\n",
       "      <td>7.088702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       finishing_position  horse_number         horse_name horse_id  \\\n",
       "24155                  11           3.0  GLORIOUS PARTNERS     A089   \n",
       "24156                  12           7.0           APPLAUSE     A023   \n",
       "\n",
       "          jockey   trainer  actual_weight  declared_horse_weight  draw  \\\n",
       "24155     T H So  D J Hall            125                   1112     9   \n",
       "24156  O Doleuze   K L Man            124                   1067     7   \n",
       "\n",
       "      length_behind_winner  ...  running_position_6   race_id  recent_6_runs  \\\n",
       "24155                9-1/4  ...                 NaN  2016-377          11/10   \n",
       "24156               10-1/2  ...                 NaN  2016-377       12/13/12   \n",
       "\n",
       "       recent_ave_rank race_distance  HorseWin  HorseRankTop3  \\\n",
       "24155        10.500000          1400         0              0   \n",
       "24156        12.333333          1400         0              0   \n",
       "\n",
       "       HorseRankTop50Percent jockey_ave_rank trainer_ave_rank  \n",
       "24155                      0        8.331781         6.638824  \n",
       "24156                      0        6.819133         7.088702  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 2 rows of the test files\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep the features we want to train our model on\n",
    "X_train = df_train[['actual_weight', 'declared_horse_weight',\n",
    "                    'draw', 'win_odds', 'jockey_ave_rank',\n",
    "                    'trainer_ave_rank', 'recent_ave_rank', 'race_distance']]\n",
    "\n",
    "y_train = df_train[['HorseWin', 'HorseRankTop3', 'HorseRankTop50Percent']]\n",
    "\n",
    "# Keep the features we want to train our model on\n",
    "X_test = df_test[['actual_weight', 'declared_horse_weight',\n",
    "                   'draw', 'win_odds','jockey_ave_rank',\n",
    "                    'trainer_ave_rank', 'recent_ave_rank', 'race_distance']]\n",
    "                    \n",
    "y_test = df_test[['HorseWin', 'HorseRankTop3', 'HorseRankTop50Percent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24155, 8)\n",
      "(24155, 3)\n",
      "(5209, 8)\n",
      "(5209, 3)\n"
     ]
    }
   ],
   "source": [
    "# View the shape of the train and test files\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HorseWin                 0.080108\n",
       "HorseRankTop3            0.239743\n",
       "HorseRankTop50Percent    0.499690\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the mean of the target variable\n",
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is data imbalance for the HorseWin and HorseRankTop3 variables, so we need to account for these later when modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify kfold cross validation\n",
    "kfold = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation mean score for Logistic Regression: \n",
      " Horse win: 0.046 \n",
      " Horse in Top 3: 0.439 \n",
      " Horse in Top 50%: 0.721\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cross validation score\n",
    "# Round the score to 3 decimal places\n",
    "\n",
    "score_lr_win = cross_val_score(lr, X_train, y_train['HorseWin'],\n",
    "                              cv = kfold, scoring = 'f1').mean()\n",
    "score_lr_win = round(score_lr_win, 3)\n",
    "\n",
    "score_lr_top3 = cross_val_score(lr, X_train,y_train['HorseRankTop3'],\n",
    "                              cv = kfold,scoring = 'f1').mean()\n",
    "score_lr_top3 = round(score_lr_top3, 3)\n",
    "\n",
    "score_lr_top50 = cross_val_score(lr,X_train, y_train['HorseRankTop50Percent'],\n",
    "                              cv = kfold,scoring = 'f1').mean()\n",
    "score_lr_top50 = round(score_lr_top50, 3)\n",
    "\n",
    "print(\"Cross Validation mean score for Logistic Regression:\",'\\n',\n",
    "      \"Horse win:\", score_lr_win,'\\n',\\\n",
    "      \"Horse in Top 3:\", score_lr_top3,'\\n',\\\n",
    "      \"Horse in Top 50%:\", score_lr_top50)\n",
    "\n",
    "# Create table to store the results\n",
    "cross_val_score_table = pd.DataFrame(columns = ['Model', 'HorseWin', 'HorseRankTop3', 'HorseRankTop50Percent'])\n",
    "\n",
    "# Add scores to the table\n",
    "cross_val_score_table = cross_val_score_table.append({'Model': 'Logistic Regression',\n",
    "                                                         'HorseWin': score_lr_win,\n",
    "                                                            'HorseRankTop3': score_lr_top3,\n",
    "                                                            'HorseRankTop50Percent': score_lr_top50},\n",
    "                                                            ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for logistic regression is: 0.35 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the classification predictions (1 or 0)\n",
    "# start time\n",
    "start_time = time.time()\n",
    "\n",
    "lr.fit(X_train, y_train['HorseWin'])\n",
    "lr_win = lr.predict(X_test)\n",
    "\n",
    "lr.fit(X_train, y_train['HorseRankTop3'])\n",
    "lr_top3 = lr.predict(X_test)\n",
    "\n",
    "lr.fit(X_train,y_train['HorseRankTop50Percent'])\n",
    "lr_top50 = lr.predict(X_test)\n",
    "\n",
    "print('Running time for logistic regression is:', round(time.time() - start_time, 3), 'seconds')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for predictions\n",
    "lr_pred = pd.DataFrame()\n",
    "lr_pred['RaceID'] = df_test['race_id']\n",
    "lr_pred['HorseID'] = df_test['horse_id']\n",
    "\n",
    "lr_pred['HorseWin'] = lr_win\n",
    "lr_pred['HorseRankTop3'] = lr_top3\n",
    "lr_pred['HorseRankTop50Percent'] = lr_top50\n",
    "\n",
    "# Write predictions into csv file.\n",
    "lr_pred.to_csv('lr_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For imbalanced data, 1 is more important than 0. The model may try to increase accuracy by predicting all 0. F1 score will be close to 0 while accuracy is close to 1. So for imbalanced data, F1 score (similarly TNR, NPV) is good choice.\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Precision P = TP / (TP + FP), probability that one classified positive instance is classified correctly.\n",
    "\n",
    "Recall R = TP / (TP +FN) , percentage of truly positive instances correctly classified. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Logistic Regression: \n",
      " Horse win: 0.068 \n",
      " Horse in Top 3: 0.349 \n",
      " Horse in Top 50%: 0.696\n"
     ]
    }
   ],
   "source": [
    "# F1 score for logistic regression\n",
    "f1_win = round(f1_score(y_test['HorseWin'], lr_win), 3)\n",
    "\n",
    "f1_top3 = round(f1_score(y_test['HorseRankTop3'], lr_top3), 3)\n",
    "\n",
    "f1_top50 = round(f1_score(y_test['HorseRankTop50Percent'], lr_top50), 3)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 score for Logistic Regression:\",'\\n',\n",
    "      \"Horse win:\", f1_win,'\\n',\\\n",
    "      \"Horse in Top 3:\", f1_top3,'\\n',\\\n",
    "      \"Horse in Top 50%:\", f1_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Logistic Regression\n",
      "Horse win                       0.068\n",
      "Horse in Top 3                  0.349\n",
      "Horse in Top 50%                0.696\n"
     ]
    }
   ],
   "source": [
    "# Create a table to compare the F1 score\n",
    "f1_score_table = pd.DataFrame({'Logistic Regression':[f1_win, f1_top3, f1_top50]},\n",
    "                                index = ['Horse win', 'Horse in Top 3', 'Horse in Top 50%'])\n",
    "\n",
    "print(f1_score_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation mean score for Gaussian Naive Bayes: \n",
      " Horse win: 0.315 \n",
      " Horse in Top 3: 0.541 \n",
      " Horse in Top 50%: 0.729\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cross validation score\n",
    "# Round the score to 3 decimal places\n",
    "\n",
    "score_gnb_win = cross_val_score(gnb, X_train, y_train['HorseWin'],\n",
    "                              cv = kfold, scoring = 'f1').mean()\n",
    "score_gnb_win = round(score_gnb_win, 3)\n",
    "\n",
    "score_gnb_top3 = cross_val_score(gnb, X_train,y_train['HorseRankTop3'],\n",
    "                              cv = kfold, scoring = 'f1').mean() \n",
    "score_gnb_top3 = round(score_gnb_top3, 3)\n",
    "\n",
    "score_gnb_top50 = cross_val_score(gnb,X_train, y_train['HorseRankTop50Percent'],\n",
    "                              cv = kfold, scoring = 'f1').mean()\n",
    "score_gnb_top50 = round(score_gnb_top50, 3)\n",
    "\n",
    "# Print the cross validation score\n",
    "print(\"Cross Validation mean score for Gaussian Naive Bayes:\",'\\n',\n",
    "      \"Horse win:\", score_gnb_win,'\\n',\\\n",
    "      \"Horse in Top 3:\", score_gnb_top3,'\\n',\\\n",
    "      \"Horse in Top 50%:\", score_gnb_top50)\n",
    "\n",
    "# Append the results to the table\n",
    "cross_val_score_table = cross_val_score_table.append({'Model': 'Gaussian Naive Bayes',\n",
    "                                                            'HorseWin': score_gnb_win,    \n",
    "                                                            'HorseRankTop3': score_gnb_top3,    \n",
    "                                                            'HorseRankTop50Percent': score_gnb_top50},\n",
    "                                                            ignore_index = True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for Gaussian Naive Bayes is: 0.017 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the classification predictions (1 or 0)\n",
    "start_time2 = time.time()\n",
    "\n",
    "gnb.fit(X_train, y_train['HorseWin'])\n",
    "gnb_win = gnb.predict(X_test)\n",
    "\n",
    "gnb.fit(X_train, y_train['HorseRankTop3'])\n",
    "gnb_top3 = gnb.predict(X_test)\n",
    "\n",
    "gnb.fit(X_train,y_train['HorseRankTop50Percent'])\n",
    "gnb_top50 = gnb.predict(X_test)\n",
    "\n",
    "print('Running time for Gaussian Naive Bayes is:', round(time.time() - start_time2, 3), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for predictions\n",
    "\n",
    "gnb_pred = pd.DataFrame()\n",
    "gnb_pred['RaceID'] = df_test['race_id']\n",
    "gnb_pred['HorseID'] = df_test['horse_id']\n",
    "\n",
    "gnb_pred['HorseWin'] = gnb_win\n",
    "gnb_pred['HorseRankTop3'] = gnb_top3\n",
    "gnb_pred['HorseRankTop50Percent'] = gnb_top50\n",
    "\n",
    "# Write predictions into csv file.\n",
    "gnb_pred.to_csv('gnb_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Gaussian Naive Bayes: \n",
      " Horse win: 0.278 \n",
      " Horse in Top 3: 0.504 \n",
      " Horse in Top 50%: 0.719\n"
     ]
    }
   ],
   "source": [
    "# F1 score for Gaussian Naive Bayes\n",
    "f1_win = round(f1_score(y_test['HorseWin'], gnb_win), 3)\n",
    "\n",
    "f1_top3 = round(f1_score(y_test['HorseRankTop3'], gnb_top3), 3)\n",
    "\n",
    "f1_top50 = round(f1_score(y_test['HorseRankTop50Percent'], gnb_top50), 3)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 score for Gaussian Naive Bayes:\",'\\n',\n",
    "        \"Horse win:\", f1_win,'\\n',\\\n",
    "        \"Horse in Top 3:\", f1_top3,'\\n',\\\n",
    "        \"Horse in Top 50%:\", f1_top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the F1 score to the table\n",
    "f1_score_table['Gaussian Naive Bayes'] = [f1_win, f1_top3, f1_top50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rfc = RandomForestClassifier(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation mean score for Random Forest Classifier: \n",
      " Horse win: 0.183 \n",
      " Horse in Top 3: 0.456 \n",
      " Horse in Top 50%: 0.709\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cross validation score\n",
    "# Round the score to 3 decimal places\n",
    "\n",
    "score_rfc_win = cross_val_score(rfc, X_train, y_train['HorseWin'],\n",
    "                                cv = kfold, scoring = 'f1').mean()\n",
    "score_rfc_win = round(score_rfc_win, 3)\n",
    "\n",
    "score_rfc_top3 = cross_val_score(rfc, X_train, y_train['HorseRankTop3'],\n",
    "                                    cv = kfold, scoring = 'f1').mean()\n",
    "score_rfc_top3 = round(score_rfc_top3, 3)\n",
    "\n",
    "score_rfc_top50 = cross_val_score(rfc, X_train, y_train['HorseRankTop50Percent'],\n",
    "                                    cv = kfold, scoring = 'f1').mean()\n",
    "score_rfc_top50 = round(score_rfc_top50, 3)\n",
    "\n",
    "# Print the cross validation score\n",
    "print(\"Cross Validation mean score for Random Forest Classifier:\",'\\n',\n",
    "        \"Horse win:\", score_rfc_win,'\\n',\\\n",
    "        \"Horse in Top 3:\", score_rfc_top3,'\\n',\\\n",
    "        \"Horse in Top 50%:\", score_rfc_top50)\n",
    "\n",
    "# Add the cross validation score to the table\n",
    "cross_val_score_table = cross_val_score_table.append({'Model': 'Random Forest Classifier',\n",
    "                                                        'HorseWin': score_rfc_win,\n",
    "                                                        'HorseRankTop3': score_rfc_top3,\n",
    "                                                        'HorseRankTop50Percent': score_rfc_top50},\n",
    "                                                        ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for Random Forest Classifier is: 6.751 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the classification predictions (1 or 0)\n",
    "start_time3 = time.time()\n",
    "\n",
    "rfc.fit(X_train, y_train['HorseWin'])\n",
    "rfc_win = rfc.predict(X_test)\n",
    "\n",
    "rfc.fit(X_train, y_train['HorseRankTop3'])\n",
    "rfc_top3 = rfc.predict(X_test)\n",
    "\n",
    "rfc.fit(X_train, y_train['HorseRankTop50Percent'])\n",
    "rfc_top50 = rfc.predict(X_test)\n",
    "\n",
    "print('Running time for Random Forest Classifier is:', round(time.time() - start_time3, 3), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for predictions\n",
    "rfc_pred = pd.DataFrame()\n",
    "rfc_pred['RaceID'] = df_test['race_id']\n",
    "rfc_pred['HorseID'] = df_test['horse_id']\n",
    "\n",
    "rfc_pred['HorseWin'] = rfc_win\n",
    "rfc_pred['HorseRankTop3'] = rfc_top3\n",
    "rfc_pred['HorseRankTop50Percent'] = rfc_top50\n",
    "\n",
    "# Write predictions into csv file.\n",
    "rfc_pred.to_csv('rfc_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Random Forest Classifier: \n",
      " Horse win: 0.168 \n",
      " Horse in Top 3: 0.377 \n",
      " Horse in Top 50%: 0.692\n"
     ]
    }
   ],
   "source": [
    "# F1 score for Random Forest Classifier\n",
    "f1_win = round(f1_score(y_test['HorseWin'], rfc_win), 3)\n",
    "f1_top3 = round(f1_score(y_test['HorseRankTop3'], rfc_top3), 3)\n",
    "f1_top50 = round(f1_score(y_test['HorseRankTop50Percent'], rfc_top50), 3)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 score for Random Forest Classifier:\",'\\n',\n",
    "        \"Horse win:\", f1_win,'\\n',\\\n",
    "        \"Horse in Top 3:\", f1_top3,'\\n',\\\n",
    "        \"Horse in Top 50%:\", f1_top50)\n",
    "\n",
    "# Append the F1 score to the table\n",
    "f1_score_table['Random Forest Classifier'] = [f1_win, f1_top3, f1_top50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>HorseWin</th>\n",
       "      <th>HorseRankTop3</th>\n",
       "      <th>HorseRankTop50Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  HorseWin  HorseRankTop3  HorseRankTop50Percent\n",
       "0       Logistic Regression     0.046          0.439                  0.721\n",
       "1      Gaussian Naive Bayes     0.315          0.541                  0.729\n",
       "2  Random Forest Classifier     0.183          0.456                  0.709"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the cross validation score table\n",
    "cross_val_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 4: Smote + Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time is: 127.59 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time the model\n",
    "start_time4 = time.time()\n",
    "\n",
    "# Smote the training data\n",
    "sm = SMOTE(random_state = 42)\n",
    "rfc = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Steps for the pipeline\n",
    "steps = [('smote', sm), ('rfc', rfc)]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "# Create the parameter grid\n",
    "\n",
    "param_grid = {'rfc__max_depth': [5, 20, 30],\n",
    "            'rfc__min_samples_leaf': [5, 10, 20]}\n",
    "            \n",
    "# Create the grid search object\n",
    "grid = GridSearchCV(pipeline, param_grid = param_grid, cv = kfold, scoring = 'f1')\n",
    "\n",
    "# Fit the grid search\n",
    "grid.fit(X_train, y_train['HorseWin'])\n",
    "\n",
    "# Print the running time\n",
    "print('Running time is:', round(time.time() - start_time4, 3), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest Classifier Parameters: {'rfc__max_depth': 20, 'rfc__min_samples_leaf': 10}\n",
      "Best score is 0.31964430969538316\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters\n",
    "print(\"Tuned Random Forest Classifier Parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Print the best score\n",
    "print(\"Best score is {}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15624/1411614540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mscore_grid_top3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_grid_top3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m score_grid_top50 = cross_val_score(grid, X_train, y_train['HorseRankTop50Percent'],\n\u001b[0m\u001b[0;32m     13\u001b[0m                                     cv = kfold, scoring = 'f1').mean()\n\u001b[0;32m     14\u001b[0m \u001b[0mscore_grid_top50\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_grid_top50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    820\u001b[0m                     )\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    823\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    477\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \"\"\"\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    970\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    456\u001b[0m             )\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate the cross validation score\n",
    "# Round the score to 3 decimal places\n",
    "\n",
    "score_grid_win = cross_val_score(grid, X_train, y_train['HorseWin'],\n",
    "                                cv = kfold, scoring = 'f1').mean()\n",
    "score_grid_win = round(score_grid_win, 3)\n",
    "\n",
    "score_grid_top3 = cross_val_score(grid, X_train, y_train['HorseRankTop3'],\n",
    "                                    cv = kfold, scoring = 'f1').mean()\n",
    "score_grid_top3 = round(score_grid_top3, 3)\n",
    "\n",
    "score_grid_top50 = cross_val_score(grid, X_train, y_train['HorseRankTop50Percent'],\n",
    "                                    cv = kfold, scoring = 'f1').mean()\n",
    "score_grid_top50 = round(score_grid_top50, 3)\n",
    "\n",
    "# Print the cross validation score\n",
    "print(\"Cross Validation mean score for RFC + Smote:\",'\\n',\n",
    "        \"Horse win:\", score_grid_win,'\\n',\\\n",
    "        \"Horse in Top 3:\", score_grid_top3,'\\n',\\\n",
    "        \"Horse in Top 50%:\", score_grid_top50)\n",
    "\n",
    "# Add the cross validation score to the table\n",
    "cross_val_score_table = cross_val_score_table.append({'Model': 'RFC + Smote',\n",
    "                                                        'HorseWin': score_grid_win,\n",
    "                                                        'HorseRankTop3': score_grid_top3,\n",
    "                                                        'HorseRankTop50Percent': score_grid_top50},\n",
    "                                                        ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for RFC + Smote is: 285.386 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the classification predictions (1 or 0)\n",
    "start_time5 = time.time()\n",
    "\n",
    "grid.fit(X_train, y_train['HorseWin'])\n",
    "grid_win = grid.predict(X_test)\n",
    "\n",
    "grid.fit(X_train, y_train['HorseRankTop3'])\n",
    "grid_top3 = grid.predict(X_test)\n",
    "\n",
    "grid.fit(X_train, y_train['HorseRankTop50Percent'])\n",
    "grid_top50 = grid.predict(X_test)\n",
    "\n",
    "print('Running time for RFC + Smote is:', round(time.time() - start_time5, 3), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for predictions\n",
    "grid_pred = pd.DataFrame()\n",
    "grid_pred['RaceID'] = df_test['race_id']\n",
    "grid_pred['HorseID'] = df_test['horse_id']\n",
    "\n",
    "grid_pred['HorseWin'] = grid_win\n",
    "grid_pred['HorseRankTop3'] = grid_top3\n",
    "grid_pred['HorseRankTop50Percent'] = grid_top50\n",
    "\n",
    "# Write predictions into csv file.\n",
    "grid_pred.to_csv('grid_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for RFC + Smote: \n",
      " Horse win: 0.303 \n",
      " Horse in Top 3: 0.537 \n",
      " Horse in Top 50%: 0.717\n"
     ]
    }
   ],
   "source": [
    "# f1 score for RFC + Smote\n",
    "f1_win = round(f1_score(y_test['HorseWin'], grid_win), 3)\n",
    "f1_top3 = round(f1_score(y_test['HorseRankTop3'], grid_top3), 3)\n",
    "f1_top50 = round(f1_score(y_test['HorseRankTop50Percent'], grid_top50), 3)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 score for RFC + Smote:\",'\\n',\n",
    "        \"Horse win:\", f1_win,'\\n',\\\n",
    "        \"Horse in Top 3:\", f1_top3,'\\n',\\\n",
    "        \"Horse in Top 50%:\", f1_top50)\n",
    "\n",
    "# Append the F1 score to the table\n",
    "f1_score_table['RFC_Smote'] = [f1_win, f1_top3, f1_top50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>HorseWin</th>\n",
       "      <th>HorseRankTop3</th>\n",
       "      <th>HorseRankTop50Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  HorseWin  HorseRankTop3  HorseRankTop50Percent\n",
       "0       Logistic Regression     0.046          0.439                  0.721\n",
       "1      Gaussian Naive Bayes     0.315          0.541                  0.729\n",
       "2  Random Forest Classifier     0.183          0.456                  0.709"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the cross validation score table\n",
    "cross_val_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <th>RFC_Smote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Horse win</th>\n",
       "      <td>0.068</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horse in Top 3</th>\n",
       "      <td>0.349</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horse in Top 50%</th>\n",
       "      <td>0.696</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Logistic Regression  Gaussian Naive Bayes  \\\n",
       "Horse win                       0.068                 0.278   \n",
       "Horse in Top 3                  0.349                 0.504   \n",
       "Horse in Top 50%                0.696                 0.719   \n",
       "\n",
       "                  Random Forest Classifier  RFC_Smote  \n",
       "Horse win                            0.168      0.303  \n",
       "Horse in Top 3                       0.377      0.537  \n",
       "Horse in Top 50%                     0.692      0.717  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing of Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "\n",
    "X_train = df_train[['actual_weight', 'declared_horse_weight',\n",
    "                    'draw','win_odds','jockey_ave_rank','trainer_ave_rank',\n",
    "                    'recent_ave_rank','race_distance']]\n",
    "\n",
    "# Define the target\n",
    "y_train = df_train['finish_time']\n",
    "\n",
    "# Convert the target to seconds\n",
    "y_train = y_train.apply(lambda x: x.split('.'))\n",
    "y_train = y_train.apply(lambda x: int(x[0])*60 + int(x[1]) + int(x[2])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    82.33\n",
       "1    82.65\n",
       "2    82.66\n",
       "3    82.66\n",
       "4    83.02\n",
       "Name: finish_time, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing set\n",
    "X_test = df_test[['actual_weight', 'declared_horse_weight',\n",
    "                    'draw', 'win_odds', 'jockey_ave_rank', 'trainer_ave_rank',\n",
    "                    'recent_ave_rank', 'race_distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target\n",
    "y_test = df_test['finish_time']\n",
    "\n",
    "# Convert the target to seconds\n",
    "y_test = y_test.apply(lambda x: x.split('.'))\n",
    "y_test = y_test.apply(lambda x: int(x[0])*60 + int(x[1]) + int(x[2])/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation:\n",
    "1. MSE\n",
    "2. Top_1: the percentage/probality when your prediction of top_1 horse(horse with shortest finish_time) for each race is actually the true top_1 horse.\n",
    "\n",
    "3. Top_3: percentage/probability when your prediction of top_1 horse for each race is actually within true top_3 horses for each race. \n",
    "\n",
    "4. Average_rank: the average true rank of top_1 horse based on your prediction over all races.\n",
    "\n",
    "For example, when you predict for 3 races and your predicted top_1 horse is actually ranking 1, 3, 5 in these races. Top_1 is 1/3, Top_3 is 2/3 and Average_Rank is 3.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 13, 25, 37, 49]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of index of Top 1 Position from the testing set\n",
    "top1_index = df_test.index[df_test['finishing_position']==1].tolist()\n",
    "print(top1_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the model\n",
    "# Takes in the prediction and evaluates with the RMSE, top1 & top3 probability and avarage rank\n",
    "\n",
    "def evaluation(y_pred):\n",
    "    top1_predict_index = []\n",
    "\n",
    "    for i in range(len(top1_index)-1):\n",
    "        \n",
    "        # Find the min value in the prediction\n",
    "        temp = np.argmin(y_pred[top1_index[i]:top1_index[i + 1]])\n",
    "        \n",
    "        top1_predict_index.append(top1_index[i] + temp)\n",
    "\n",
    "    temp0 = np.argmin(y_pred[top1_index[len(top1_index) - 1]:])\n",
    "    top1_predict_index.append(top1_index[len(top1_index) - 1] + temp0)\n",
    "\n",
    "    rmse = math.sqrt(sum((np.array(y_pred) - np.array(y_test)) ** 2)) / len(y_test)\n",
    "    rmse = round(rmse, 3)\n",
    "\n",
    "    top_1 = float(len(set(top1_predict_index) & set(top1_index))) / len(top1_predict_index)\n",
    "    top_1  = round(top_1, 3)\n",
    "\n",
    "    top_3 = (df_test['finishing_position'][top1_predict_index].tolist().count(1) + df_test['finishing_position'][top1_predict_index].tolist().count(2)\\\n",
    "          + df_test['finishing_position'][top1_predict_index].tolist().count(3)) / float(len(top1_predict_index))\n",
    "    top_3 = round(top_3, 3)\n",
    "\n",
    "    avg_rank = sum(df_test['finishing_position'][top1_predict_index]) / float(len(top1_predict_index))\n",
    "    avg_rank = round(avg_rank, 3)\n",
    "\n",
    "    return (rmse, top_1, top_3, avg_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15624/2802886081.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mridge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mridge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Ridge' is not defined"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the model\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Standardize the testing set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make prediction\n",
    "ridge_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation(ridge_pred)\n",
    "\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Gradient Boosting Regression Tree Model (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression Tree Model is a generalization of boosting technique to arbitrary differentiable loss functions.\n",
    "It is used here becauese of its natural handling of data of mixed type, great predictive power and robustness to outliers in output space (via robust loss functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Parameters for GBRT:\n",
    "Loss function: It has various loss functions including ls, lad, huber, quantile. Choose loss=’qunatile’, since for default values of other parameters, this loss function performs best according to TOP_1 and TOP_3 evaluation statistics.\n",
    "\n",
    "learning_rate: controls the contribution of each weak classifier (tree).\n",
    "\n",
    "n_estimators: represents the number of weak learners (tree). Since boosting combines the output of many weak classifiers, the larger n_estimators, the more robust the model is and the better results are.\n",
    "\n",
    "max_depth: maximum nodes of the tree\n",
    "\n",
    "I chose learning_rate = 0.01, n_estimators = 10, max_depth = 2, since I found by assigning these three values to parameters, TOP_1 = 0.99, TOP_3 = 1, which performs the best in predicting winner of horse races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=10, learning_rate=0.01,\n",
    "                                 random_state=42, loss='quantile')\n",
    "\n",
    "# Fit the model\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "gbrt_pred = gbrt.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "gbrt_eval = evaluation(gbrt_pred)\n",
    "\n",
    "# Print the evaluation result\n",
    "print(\"RMSE:\", gbrt_eval[0],'\\n',\\\n",
    "        \"Top 1 Probability:\", gbrt_eval[1],'\\n',\\\n",
    "        \"Top 3 Probability:\", gbrt_eval[2],'\\n',\\\n",
    "        \"Average Rank:\", gbrt_eval[3])\n",
    "\n",
    "# Original model\n",
    "#gbrt_model = GradientBoostingRegressor(loss = 'quantile',learning_rate = 0.01, n_estimators = 10, max_depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean-squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Instantiate the model\n",
    "rmse_train = mean_squared_error(y_train, gbrt.predict(X_train))\n",
    "rmse_test = mean_squared_error(y_test, gbrt.predict(X_test))\n",
    "\n",
    "# Print the RMSE\n",
    "print(\"RMSE for training set:\", rmse_train,'\\n',\\\n",
    "        \"RMSE for testing set:\", rmse_test)\n",
    "\n",
    "# Generalization error\n",
    "print(\"Generalization Error:\", (rmse_test - rmse_train)/rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to a dataframe\n",
    "gbrt_pred = pd.DataFrame(gbrt_pred)\n",
    "\n",
    "# Save the predictions to a csv file\n",
    "gbrt_pred.to_csv('gbrt_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Betting Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betting strategy is to bet all $1 for the predicted winning horse for each race. \n",
    "\n",
    "Concretely, if our prediction is correct for the winning horse, we will receive $1 × odds money. \n",
    "\n",
    "Otherwise, we will lose $1. \n",
    "\n",
    "The final result is positive if we win some money and negative if we lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4 classification models, if there are more than 1 HorseWin in a race in predictions, I will choose the one with smallest odds, since as odds increase, winning probability decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv('testing.csv')\n",
    "champion_index = testing[testing['HorseWin'] == 1].index.tolist()\n",
    "champion_odds = testing[testing['HorseWin'] == 1]['win_odds'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_range_in_list(li, min, max):\n",
    "    ctr = 0\n",
    "    for x in li:\n",
    "        if min <= x <= max:\n",
    "            ctr += 1\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ele_in_list(li, min, max):\n",
    "    ele = []\n",
    "    for x in li:\n",
    "        if min <= x <= max:\n",
    "            ele.append(x)\n",
    "    return ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betting_result(champion_odds,champion_index,prediction):\n",
    "    money=0\n",
    "    for i in range(len(champion_index)-1):\n",
    "        ctr= count_range_in_list(prediction,champion_index[i],champion_index[i+1]-1)\n",
    "        if ctr==0:\n",
    "            money=money-1\n",
    "        elif ctr==1:\n",
    "            money=money-1+champion_odds[i]\n",
    "        else:\n",
    "            ele_list=ele_in_list(prediction,champion_index[i],champion_index[i+1]-1)\n",
    "            if min(ele_list)==champion_index[i]:\n",
    "                money=money-1+champion_odds[i]\n",
    "            else:\n",
    "                money=money-1\n",
    "    ctr = count_range_in_list(prediction, champion_index[len(champion_index)-1],len(testing['HorseWin'])-1)\n",
    "    if ctr == 0:\n",
    "        money = money - 1\n",
    "    elif ctr == 1:\n",
    "        money = money - 1 + champion_odds[len(champion_index)-1]\n",
    "    else:\n",
    "        ele_list = ele_in_list(prediction, champion_index[len(champion_index)-1], len(testing['HorseWin'])-1)\n",
    "        if min(ele_list)==champion_index[len(champion_index)-1]:\n",
    "            money = money - 1 + champion_odds[len(champion_index)-1]\n",
    "        else:\n",
    "            money = money - 1\n",
    "    return money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pd.read_csv('lr_predictions.csv')\n",
    "lr_index = lr[lr['HorseWin'] == 1].index.tolist()\n",
    "print('Betting result for Logistic Regression model:',betting_result(champion_odds,champion_index,lr_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = pd.read_csv('nb_predictions.csv')\n",
    "nb_index = nb[nb['HorseWin'] == 1].index.tolist()\n",
    "print('Betting result for Naive Bayes model:', betting_result(champion_odds,champion_index,nb_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pd.read_csv('rf_predictions.csv')\n",
    "rf_index = rf[rf['HorseWin'] == 1].index.tolist()\n",
    "print('Betting result for Random Forest model:',betting_result(champion_odds,champion_index,rf_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = pd.read_csv('svm_predictions.csv')\n",
    "svm_index = svm[svm['HorseWin'] == 1].index.tolist()\n",
    "print('Betting result for SVM model:',betting_result(champion_odds,champion_index,svm_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 4 regression models, I choose the horse with shortest predicted finish_time as the unique winning horse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(predict):\n",
    "    top1_predict_index = []\n",
    "    for i in range(len(champion_index)-1):\n",
    "        temp = np.argmin(predict[champion_index[i]:champion_index[i + 1]])\n",
    "        top1_predict_index.append(champion_index[i]+temp)\n",
    "    temp0 = np.argmin(predict[champion_index[len(champion_index) - 1]:])\n",
    "    top1_predict_index.append(champion_index[len(champion_index) - 1] + temp0)\n",
    "    return top1_predict_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_prediction = pd.read_csv('reg_prediction.csv')\n",
    "reg_svr = reg_prediction['svr_predict']\n",
    "reg_svr_norm = reg_prediction['svr_predict_norm']\n",
    "reg_gbrt = reg_prediction['gbrt_predict']\n",
    "reg_gbrt_norm = reg_prediction['gbrt_predict_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_index = prediction(reg_svr)\n",
    "svr_norm_index = prediction(reg_svr_norm)\n",
    "gbrt_index = prediction(reg_gbrt)\n",
    "gbrt_norm_index = prediction(reg_gbrt_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Betting result for SVR model:',betting_result(champion_odds,champion_index,svr_index))\n",
    "print('Betting result for SVR (Normalized) model:',betting_result(champion_odds,champion_index,svr_norm_index))\n",
    "print('Betting result for GBRT model:',betting_result(champion_odds,champion_index,gbrt_index))\n",
    "print('Betting result for GBRT (Normalized) model:',betting_result(champion_odds,champion_index,gbrt_norm_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems 2 regression algorithms perform well and normalization improves performance of SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement:\n",
    "\n",
    "I set threshold for the average rank and odds. For example, we only bet the horse whose odd is in the smallest 5, and recent_ave_rank is also in smallest 5. This means we decreases the risk of betting in horses with bad recent performance. If the horse cannot satisfy the criteria, we do not bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_betting(champion_odds,champion_index,prediction):\n",
    "    money = 0\n",
    "    for i in range(len(champion_index) - 1):\n",
    "        ctr = count_range_in_list(prediction, champion_index[i], champion_index[i + 1] - 1)\n",
    "        if ctr >= 1:\n",
    "            temp_odds = testing['win_odds'].tolist()[champion_index[i]:champion_index[i + 1]]\n",
    "            temp_ave_rank = testing['recent_ave_rank'].tolist()[champion_index[i]:champion_index[i + 1]]\n",
    "            seq_odds = sorted(temp_odds)\n",
    "            seq_ave_rank = sorted(temp_ave_rank)\n",
    "            ele_list = ele_in_list(prediction,champion_index[i],champion_index[i+1]-1)\n",
    "            if (seq_odds.index(testing['win_odds'][ele_list[0]]) <= 5) and (seq_ave_rank.index(testing['recent_ave_rank'][ele_list[0]]) <= 5):\n",
    "                money = money - 1\n",
    "                if ele_list[0] == champion_index[i]:\n",
    "                    money = money + champion_odds[i]\n",
    "    return money\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Improved betting result for Logistic Regression model:',imp_betting(champion_odds,champion_index,lr_index))\n",
    "print('Improved betting result for Naive Bayes model:',imp_betting(champion_odds,champion_index,nb_index))\n",
    "print('Improved betting result for Random Forest model:',imp_betting(champion_odds,champion_index,rf_index))\n",
    "print('Improved betting result for SVR model:',imp_betting(champion_odds,champion_index,svr_index))\n",
    "print('Improved betting result fo GBRT model:',imp_betting(champion_odds,champion_index,gbrt_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it seems that setting threshold cannot improve the results for those method whose results are already positive. It only decreases losses by decreasing risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Line Chart of Recent Racing Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the history racing result of some specific horse.\n",
    "\n",
    "Interactive: takes a horse ID as input, and outputs a line chart that shows the finishing positions of 6 recent races that the horse attended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linechart(horse_id):\n",
    "    recent_6_runs = training[training.horse_id == horse_id]['recent_6_runs'][-1:].tolist()[0]\n",
    "\n",
    "    recent_6_runs = list(map(int,recent_6_runs.split('/')))[::-1]\n",
    "    print(recent_6_runs)\n",
    "    game_id = training[training.horse_id == horse_id][['race_id']][-6:]\n",
    "    print(game_id)\n",
    "    plt.plot(game_id.iloc[:,0], recent_6_runs, marker = '+')\n",
    "    plt.xlabel('Game_id')\n",
    "    plt.ylabel('Ranks of recent 6 runs')\n",
    "    plt.title('Line Chart of recent 6 runs'+'- Horse ' + horse_id)\n",
    "    plt.ylim((0, 14))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('training.csv')\n",
    "horse_id = 'S047'\n",
    "linechart(horse_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Scatter Plot of Win Rate and Number of Wins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis is the win rate, and the y-axis is the number of wins. \n",
    "\n",
    "Set a threshold and label the name of the horses (or jockeys) who reach the threshold. E.g., if a horse’s win rate is larger than 0.5, and wins more than 4 games, then you should annotate the point of this horse with its name. \n",
    "\n",
    "Goal: to find the “best” horse and the “best” jockey. Intuitively, the “best” one should have a high win rate and have won a large number of games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('training.csv')\n",
    "jockey = training.jockey.unique()\n",
    "a = pd.DataFrame()\n",
    "a['jockey'] = jockey\n",
    "a['no_win'] = 0\n",
    "a['win_rate'] = 0.0\n",
    "for i in range(len(jockey)):\n",
    "    ranks = training[training.jockey == jockey[i]]['finishing_position'].tolist()\n",
    "    a['no_win'][i] = ranks.count(1)\n",
    "    a['win_rate'][i] = ranks.count(1) / float(len(ranks))\n",
    "\n",
    "horse = training.horse_name.unique()\n",
    "b = pd.DataFrame()\n",
    "b['horse'] = horse\n",
    "b['no_win'] = 0\n",
    "b['win_rate'] = 0.0\n",
    "for i in range(len(jockey)):\n",
    "    ranks=training[training.horse_name == horse[i]]['finishing_position'].tolist()\n",
    "    b['no_win'][i] = ranks.count(1)\n",
    "    b['win_rate'][i] = ranks.count(1) / float(len(ranks))\n",
    "\n",
    "figure(num = None, figsize = (12, 12), dpi = 90, facecolor = 'w', edgecolor = 'k')\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(a['win_rate'],a['no_win'],alpha = 0.3)\n",
    "plt.title('Scatter plot for jockeys')\n",
    "plt.xlabel('Win Rate')\n",
    "plt.ylabel('Number of Wins')\n",
    "for i in range(len(jockey)):\n",
    "    if a['no_win'][i] >= 10 and a['win_rate'][i] >= 0.06:\n",
    "        plt.annotate(a['jockey'][i],(a['win_rate'][i],a['no_win'][i]),size = 7)\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(b['win_rate'],b['no_win'],alpha=0.3)\n",
    "plt.title('Scatter plot for horses')\n",
    "plt.xlabel('Win Rate')\n",
    "plt.ylabel('Number of Wins')\n",
    "for i in range(len(horse)):\n",
    "    if b['no_win'][i] >= 2 and b['win_rate'][i] >= 0.15:\n",
    "        plt.annotate(b['horse'][i],(b['win_rate'][i],b['no_win'][i]),size = 7)\n",
    "        \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best jockey is J Moreira. Since he has the highest number of wins and very high win rate.\n",
    "\n",
    "\n",
    "The best horse is Romantic Cash, since it has the highest win rate and its ranks are very stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Pie Chart of the Draw Bias Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie chart is a way to visualize the distribution of categorical data\n",
    "\n",
    "#### Goal: explore the effect of draw bias in horse racing. \n",
    "\n",
    "The draw refers to the stall a horse will start the race from. The draw is normally chosen at random on the day the horses are declared to run. Obviously, the inside lane would hold an edge over the field as they have a shorter distance to the bend, in comparison to the other lanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('training.csv')\n",
    "\n",
    "win_prob = []\n",
    "for i in range(1,16,1):\n",
    "    win_prob.append(training[training.draw == i]['finishing_position'].tolist().count(1) / float(len(training[training.draw == i])))\n",
    "\n",
    "print(win_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15']\n",
    "figure(num = None, figsize = (8, 8), dpi = 90, facecolor = 'w', edgecolor = 'k')\n",
    "plt.pie(win_prob,labels = labels,autopct = '%1.1f%%', colors = sns.color_palette(\"cubehelix\"))\n",
    "plt.title('Pie Chart of the Draw Bias Effect (Number represents No. of lane the horse will run)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low draws indeed have a considerable advantage, as we can see that as draw increases, the winning probability decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Bar Chart of the Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use random forest classifier to evaluate the importance of the features, which measures how much each feature decreases the weighted impurity in a tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('training.csv')\n",
    "rf_model = RandomForestClassifier()\n",
    "X_train = training[['actual_weight','declared_horse_weight','draw','win_odds','jockey_ave_rank','trainer_ave_rank',\n",
    "'recent_ave_rank','race_distance']]\n",
    "y_train = training[['HorseWin','HorseRankTop3','HorseRankTop50Percent']]\n",
    "rf_model.fit(X_train,y_train['HorseWin'])\n",
    "features = 'actual_weight','declared_horse_weight','draw','win_odds','jockey_ave_rank','trainer_ave_rank','recent_ave_rank','race_distance'\n",
    "importance = rf_model.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "print(importance[indices])\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num = None, figsize = (8, 6), dpi = 90, facecolor = 'w', edgecolor = 'k')\n",
    "plt.bar(range(len(features)),importance[indices],color = sns.color_palette(\"RdBu_r\", 8))\n",
    "plt.xticks(range(len(features)),features)\n",
    "plt.xlabel('Feature names')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Bar Chart of the Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We find that actual_weight, declared_horse_weight and draw affect the most, while race_distance has the least effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Visualize SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is hard to visualize high-dimensional data, for the input data X, we only consider these two features: recent_rank and jockey_ave_rank. Also, for the target y, we only care about whether the finishing position is in top 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('training.csv')\n",
    "X = training[['recent_ave_rank','jockey_ave_rank']]\n",
    "y = training['HorseRankTop50Percent']\n",
    "svm_model = SVC(kernel = 'linear')\n",
    "svm_model.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h = .02):\n",
    "\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(clf, xx, yy, **params):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = plt.contourf(xx, yy, Z, **params)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, X1 = X['recent_ave_rank'], X['jockey_ave_rank']\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "figure(num = None, figsize = (8, 6), dpi = 90, facecolor = 'w', edgecolor = 'k')\n",
    "plot_contours(svm_model,xx, yy, alpha = 0.8)\n",
    "plt.scatter(X0, X1, c = y,  s = 20, edgecolors = 'k')\n",
    "plt.title('Visualized SVM')\n",
    "plt.xlabel('Recent average rank')\n",
    "plt.ylabel('Jockey average rank')\n",
    "patch = mpatches.Patch(color = 'purple',label = 'SVC(kernel=\\'linear\\')')\n",
    "plt.legend(handles = [patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel seems not bad in two-feature SVM classification. But there are still plenty of points cross the margin which cannot be classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc902f5f9f1b4ac8244fa2d1a71d10e1bb8b95bd909217946ac119cb82bcf206"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
